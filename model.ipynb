{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912be032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from random import *\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, Add,Concatenate,Bidirectional, GRU,Layer,  Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480bc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetnames = [\"ALKBH5_Baltz2012\",\"C17ORF85_Baltz2012\",\"C22ORF28_Baltz2012\",\"CAPRIN1_Baltz2012\",\"CLIPSEQ_AGO2\",\n",
    "\"CLIPSEQ_ELAVL1\",\"CLIPSEQ_SFRS1\",\"ICLIP_HNRNPC\",\"ICLIP_TDP43\",\"ICLIP_TIA1\",\"ICLIP_TIAL1\",\n",
    "\"PARCLIP_AGO1234\",\"PARCLIP_ELAVL1\",\"PARCLIP_ELAVL1A\",\"PARCLIP_EWSR1\",\"PARCLIP_FUS\",\"PARCLIP_HUR\",\n",
    "\"PARCLIP_IGF2BP123\",\"PARCLIP_MOV10_Sievers\",\"PARCLIP_PUM2\",\"PARCLIP_QKI\",\"PARCLIP_TAF15\",\"PTBv1\",\"ZC3H7B_Baltz2012\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c3992",
   "metadata": {},
   "source": [
    "## Part1：Pre-training mask data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ad3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFa(fa):\n",
    "    with open(fa,'r') as FA:\n",
    "        seqName,seq,struct_seq='','',''\n",
    "        while 1:\n",
    "            line=FA.readline()\n",
    "            line=line.strip('\\n')\n",
    "            if (line.startswith('>') or not line) and seqName:\n",
    "                yield((seqName,seq,struct_seq))\n",
    "            if line.startswith('>'):\n",
    "                seqName = line[1:]\n",
    "                seq=''\n",
    "                struct_seq=''\n",
    "            elif line.startswith('(') or line.startswith('.'):\n",
    "                struct_seq+=line\n",
    "            else:\n",
    "                seq+=line\n",
    "            if not line:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6896eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for k-mer lemmatising the data\n",
    "\n",
    "def seq_to_kmer(sentence, kmer):\n",
    "    \n",
    "    ans = ''\n",
    "    for i in range(0,len(sentence)-kmer+1):\n",
    "        ans = ans + sentence[i:i+kmer] + ' '\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9adbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq_dictionary(kmer):\n",
    "\n",
    "    ans = '[MASK] '\n",
    "    nucleotide_list = ['a', 'u', 'c', 'g']\n",
    "    temp = [''.join(x) for x in itertools.product(*[nucleotide_list] * kmer)]\n",
    "    for i in temp:\n",
    "        ans = ans + i + ' '\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2795a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_pre_training(kmer):\n",
    "    # 1.Read pre-training data\n",
    "    training_seq_positives = []\n",
    "    training_seq_negatives = []\n",
    "    training_labels = []\n",
    "\n",
    "    for datasetname in datasetnames:\n",
    "        # Read the positive example\n",
    "        fa = \"Your own address.\"\n",
    "        for seqName, seq, struct_seq in readFa(fa):\n",
    "            if len(seq) > 501:\n",
    "                continue\n",
    "            training_seq_positives.append(seq_to_kmer(seq, kmer))\n",
    "\n",
    "        # Read the negative example\n",
    "        fa = \"Your own address.\"\n",
    "        for seqName, seq, struct_seq in readFa(fa):\n",
    "            if len(seq) > 501:\n",
    "                continue\n",
    "            training_seq_negatives.append(seq_to_kmer(seq, kmer))\n",
    "        \n",
    "\n",
    "    # 2.Initialising the splitter\n",
    "    tokenizer_seq = Tokenizer()\n",
    "    tokenizer_seq.fit_on_texts([init_seq_dictionary(kmer)])\n",
    "    word_dict = tokenizer_seq.word_index\n",
    "    number_dict = {i + 1: w for i, w in enumerate(word_dict)}\n",
    "\n",
    "    # 3.Serialise statements using a disambiguator, words in non-disambiguators (subsequences with N) are not transformed\n",
    "    training_seq_positives = tokenizer_seq.texts_to_sequences(training_seq_positives)\n",
    "    training_seq_negatives = tokenizer_seq.texts_to_sequences(training_seq_negatives)\n",
    "\n",
    "    # 4.Constructing masks (mask sequence, masked-off bits corresponding to values, masked-off positions)\n",
    "    max_pred = 75  # Maximum number of words predicted\n",
    "    vocab_size = len(word_dict) + 1\n",
    "    maxlen = 500  # Length after sequence padding\n",
    "\n",
    "    mask_sequence = []\n",
    "    mask_tokens = []\n",
    "    mask_pos = []\n",
    "\n",
    "    # Constructing the Positive Case Mask\n",
    "    for input_ids in training_seq_positives:\n",
    "        # MASK LM\n",
    "        n_pred = int(round(len(input_ids) * (0.15 / kmer)))\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)]\n",
    "        shuffle(cand_maked_pos)  \n",
    "        array = cand_maked_pos[:n_pred]\n",
    "        array.sort()\n",
    "\n",
    "        masked_pos = []\n",
    "        for i in range(n_pred - 1):\n",
    "            flag = 0\n",
    "            for j in range(kmer):\n",
    "                if array[i] + j + 1 == array[i + 1]:\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(kmer):\n",
    "                    masked_pos.append(array[i] + j)\n",
    "\n",
    "        masked_tokens = []\n",
    "        for pos in masked_pos:\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            input_ids[pos] = word_dict['mask']  # make mask\n",
    "\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "\n",
    "        n_pad = max_pred - len(masked_pos)\n",
    "        masked_tokens.extend([0] * n_pad)\n",
    "        masked_pos.extend([499] * n_pad)\n",
    "\n",
    "        mask_sequence.append(input_ids)\n",
    "        mask_tokens.append(masked_tokens)\n",
    "        mask_pos.append(masked_pos)\n",
    "        training_labels.append(1)\n",
    "\n",
    "    # Negative examples do not perform MLM tasks\n",
    "    for input_ids in training_seq_negatives:\n",
    "        # MASK LM\n",
    "        n_pred = int(round(len(input_ids) * (0.15 / kmer)))\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)]\n",
    "        shuffle(cand_maked_pos)\n",
    "        array = cand_maked_pos[:n_pred]\n",
    "        array.sort()\n",
    "        position = []\n",
    "        for i in range(n_pred - 1):\n",
    "            flag = 0\n",
    "            for j in range(kmer):\n",
    "                if array[i] + j + 1 == array[i + 1]:\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(kmer):\n",
    "                    position.append(array[i] + j)\n",
    "\n",
    "        for pos in position:\n",
    "            input_ids[pos] = word_dict['mask']  # make mask\n",
    "\n",
    "        masked_tokens = []\n",
    "        masked_pos = []\n",
    "\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "\n",
    "        masked_tokens.extend([0] * max_pred)\n",
    "        masked_pos.extend([499] * max_pred)\n",
    "\n",
    "        mask_sequence.append(input_ids)\n",
    "        mask_tokens.append(masked_tokens)\n",
    "        mask_pos.append(masked_pos)\n",
    "        training_labels.append(0)\n",
    "\n",
    "    # 6.Arrayed Lists\n",
    "    mask_sequence = np.array(mask_sequence)\n",
    "    mask_pos = np.array(mask_pos)\n",
    "    mask_tokens = np.array(mask_tokens)\n",
    "    labels = np.array(training_labels)\n",
    "\n",
    "\n",
    "    return mask_sequence, mask_tokens, mask_pos, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b3925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_fine_tuning(datasetname, kmer):\n",
    "    # 1.Read the training set\n",
    "    training_seq_positives = []\n",
    "    training_labels_positives = []\n",
    "\n",
    "    fa = \"RBP-24/\" + datasetname + \".train.positives.fa\"\n",
    "    for seqName, seq, struct_seq in readFa(fa):\n",
    "        training_seq_positives.append(seq)\n",
    "        training_labels_positives.append(1)\n",
    "\n",
    "    training_seq_negatives = []\n",
    "    training_labels_negatives = []\n",
    "\n",
    "    fa = \"RBP-24/\" + datasetname + \".train.negatives.fa\"\n",
    "    for seqName, seq, struct_seq in readFa(fa):\n",
    "        training_seq_negatives.append(seq)\n",
    "        training_labels_negatives.append(0)\n",
    "\n",
    "    training_seq = []\n",
    "    training_labels = []\n",
    "\n",
    "    while len(training_seq_positives) != 0:\n",
    "        training_seq.append(training_seq_positives.pop())\n",
    "        training_labels.append(training_labels_positives.pop())\n",
    "        if len(training_seq_negatives) != 0:\n",
    "            training_seq.append(training_seq_negatives.pop())\n",
    "            training_labels.append(training_labels_negatives.pop())\n",
    "    while len(training_seq_negatives) != 0:\n",
    "        training_seq.append(training_seq_negatives.pop())\n",
    "        training_labels.append(training_labels_negatives.pop())\n",
    "\n",
    "    # 2.Read test set\n",
    "    testing_seq_positives = []\n",
    "    testing_labels_positives = []\n",
    "\n",
    "    fa = \"RBP-24/\" + datasetname + \".ls.positives.fa\"\n",
    "    for seqName, seq, struct_seq in readFa(fa):\n",
    "        testing_seq_positives.append(seq)\n",
    "        testing_labels_positives.append(1)\n",
    "\n",
    "    testing_seq_negatives = []\n",
    "    testing_labels_negatives = []\n",
    "\n",
    "    fa = \"RBP-24/\" + datasetname + \".ls.negatives.fa\"\n",
    "    for seqName, seq, struct_seq in readFa(fa):\n",
    "        testing_seq_negatives.append(seq)\n",
    "        testing_labels_negatives.append(0)\n",
    "\n",
    "    testing_seq = []\n",
    "    testing_labels = []\n",
    "\n",
    "    while len(testing_seq_positives) != 0:\n",
    "        testing_seq.append(testing_seq_positives.pop())\n",
    "        testing_labels.append(testing_labels_positives.pop())\n",
    "        if len(testing_seq_negatives) != 0:\n",
    "            testing_seq.append(testing_seq_negatives.pop())\n",
    "            testing_labels.append(testing_labels_negatives.pop())\n",
    "\n",
    "    while len(testing_seq_negatives) != 0:\n",
    "        testing_seq.append(testing_seq_negatives.pop())\n",
    "        testing_labels.append(testing_labels_negatives.pop())\n",
    "\n",
    "    # 3.Sequence data processing\n",
    "    training_sequence = []\n",
    "\n",
    "    for sentence in training_seq:\n",
    "        temp = seq_to_kmer(sentence, kmer)\n",
    "        training_sequence.append(temp)\n",
    "\n",
    "    testing_sequence = []\n",
    "\n",
    "    for sentence in testing_seq:\n",
    "        temp = seq_to_kmer(sentence, kmer)\n",
    "        testing_sequence.append(temp)\n",
    "\n",
    "    # 4.Serialising statements using a splitter\n",
    "    tokenizer_seq = Tokenizer()\n",
    "    tokenizer_seq.fit_on_texts([init_seq_dictionary(kmer)])\n",
    "\n",
    "    max_length_seq = 500\n",
    "    trunc_type = 'post'\n",
    "    padding_type = 'post'\n",
    "\n",
    "    training_sequences = tokenizer_seq.texts_to_sequences(training_sequence)\n",
    "    training_seq_padded = pad_sequences(training_sequences, maxlen=max_length_seq, padding=padding_type,\n",
    "                                        truncating=trunc_type)\n",
    "\n",
    "    testing_sequences = tokenizer_seq.texts_to_sequences(testing_sequence)\n",
    "    testing_seq_padded = pad_sequences(testing_sequences, maxlen=max_length_seq, padding=padding_type,\n",
    "                                       truncating=trunc_type)\n",
    "\n",
    "    # 5.Arrayed Lists\n",
    "\n",
    "    training_seq_padded = np.array(training_seq_padded)\n",
    "    training_labels = np.array(training_labels)\n",
    "\n",
    "    testing_seq_padded = np.array(testing_seq_padded)\n",
    "    testing_labels = np.array(testing_labels)\n",
    "\n",
    "    return training_seq_padded, training_labels, testing_seq_padded, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d44220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def struct_to_seq(struct):\n",
    "    ans = ''\n",
    "    stack = []\n",
    "    flag = 0\n",
    "\n",
    "    for index in range(0,len(struct)):\n",
    "        if struct[index] == '(':\n",
    "            ans = ans + 'S'\n",
    "            stack.append('(')\n",
    "            flag = 0\n",
    "        elif struct[index] == ')':\n",
    "            ans = ans + 'S'\n",
    "            stack.pop()\n",
    "            flag = 1\n",
    "        else:\n",
    "            if len(stack) == 0:\n",
    "                if struct.count('(',0,index) == 0 or struct.count(')',index,len(struct)) == 0:\n",
    "                    ans = ans + 'F'\n",
    "                else:\n",
    "                    ans = ans + 'J'\n",
    "            else:\n",
    "                if flag == 1:\n",
    "                    ans = ans + 'M'\n",
    "                else:\n",
    "                    for i in range(index, len(struct)):\n",
    "                        if struct[i] == ')':\n",
    "                            ans = ans + 'H'\n",
    "                            break\n",
    "                        elif struct[i] == '(':\n",
    "                            ans = ans + 'M'\n",
    "                            break\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "455ae558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for k-mer lemmatising the data\n",
    "def struct_to_kmer(sentence, kmer):\n",
    "    \n",
    "    ans = ''\n",
    "\n",
    "    for i in range(0,len(sentence)-kmer+1):\n",
    "        ans = ans + sentence[i:i+kmer] + ' '\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1b7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_struct_dictionary(kmer):\n",
    "\n",
    "    ans = '[MASK] '\n",
    "    nucleotide_list = ['f', 'h', 'j', 'm','s']\n",
    "    temp = [''.join(x) for x in itertools.product(*[nucleotide_list] * kmer)]\n",
    "    for i in temp:\n",
    "        ans = ans + i + ' '\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af396eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_from_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_tokenizer = Tokenizer.from_file(\"structBPE.json\")\n",
    "word_dict=struct_tokenizer.get_vocab()\n",
    "dict_ans=''\n",
    "input_struct_vocab_size=len(struct_tokenizer.get_vocab())\n",
    "for i in range(1, len_vocab):\n",
    "    result = get_key_from_value(word_dict, i)\n",
    "    dict_ans = dict_ans + result + ' '\n",
    "print(dict_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bb3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_pre_training_only_struct(kmer):\n",
    "    # 1.Read pre-training data\n",
    "    training_struct_positives = []\n",
    "    training_struct_negatives = []\n",
    "\n",
    "    for datasetname in datasetnames:\n",
    "        # Read the positive example\n",
    "        fa = \"Your own address.\"\n",
    "        for seqName, seq, struct_seq in readFa(fa):\n",
    "            if len(seq) > 501:\n",
    "                continue\n",
    "            index = seq.find(\"N\")   \n",
    "            struct_seq = struct_seq[:index]  \n",
    "            struct_seq=struct_to_seq(struct_seq)\n",
    "            training_struct_positives.append(struct_to_kmer(struct_seq, kmer))\n",
    "            \n",
    "\n",
    "        # Read the negative example\n",
    "        fa = \"Your own address.\"\n",
    "        for seqName, seq, struct_seq in readFa(fa):\n",
    "            if len(seq) > 501:\n",
    "                continue\n",
    "            training_struct_negatives.append([])                    \n",
    "\n",
    "    # 2.Initialising the splitter\n",
    "    tokenizer_struct = Tokenizer()\n",
    "    tokenizer_struct.fit_on_texts([dict_ans])\n",
    "\n",
    "    # 3.Serialise statements using a disambiguator, words in non-disambiguators (subsequences with N) are not transformed\n",
    "    training_struct_positives = tokenizer_struct.texts_to_sequences(training_struct_positives)\n",
    "\n",
    "    # 4 Constructing masks (mask sequence, masked-off bits corresponding to values, masked-off positions)\n",
    "    maxlen = 500  # Length after sequence padding\n",
    "    struce_pos=[]\n",
    "    struct_ped=[]\n",
    "    \n",
    "    # Constructing the Positive Case Mask\n",
    "    for input_ids in training_struct_positives:\n",
    "\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        \n",
    "        struct_ped.append(input_ids)\n",
    "        temp=list(range(500)) \n",
    "        struce_pos.append(temp)\n",
    "        \n",
    "    # Negative examples do not perform MLM tasks\n",
    "    for input_ids in training_struct_negatives:\n",
    "\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        \n",
    "        struct_ped.append(input_ids)\n",
    "        struce_pos.append([499] * 500)\n",
    "        \n",
    "\n",
    "    # 6.Arrayed Lists\n",
    "    struce_pos=np.array(struce_pos)\n",
    "    struct_ped=np.array(struct_ped)\n",
    "\n",
    "    return struct_ped, struce_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48d656",
   "metadata": {},
   "source": [
    "## Part2：Model Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba01fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18c7d2",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be62dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6784a",
   "metadata": {},
   "source": [
    "### Scaled dot product attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e2a2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd8a17",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4afae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "       \n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k)  \n",
    "        v = self.wv(v)  \n",
    "\n",
    "        q = self.split_heads(q, batch_size)  \n",
    "        k = self.split_heads(k, batch_size)  \n",
    "        v = self.split_heads(v, batch_size)  \n",
    "\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])  \n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  \n",
    "\n",
    "        output = self.dense(concat_attention)  \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea2f5d",
   "metadata": {},
   "source": [
    "### Gelu activation function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7184c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "        return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccd1d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(dff)\n",
    "        self.activation = GELU()\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  \n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  \n",
    "\n",
    "        ffn_output = self.dense1(out1)  \n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.dense2(ffn_output)  \n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  \n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb380c",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3851907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_vocab_size, **kwargs):\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.tok_embed = tf.keras.layers.Embedding(input_vocab_size, d_model, name='tok_embed')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.tok_embed(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ba4c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maxlen, rate=0.1, name='encoder'):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.pos_encoding = positional_encoding(maxlen, d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7020b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_indexes(input, positions):\n",
    "    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "    sequence_shape = tf.shape(input)\n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_len = sequence_shape[1]\n",
    "    d_model = sequence_shape[2]\n",
    "\n",
    "    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_len, [-1, 1])\n",
    "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "    flat_input_tensor = tf.reshape(input, [batch_size * seq_len, d_model])\n",
    "    output_tensor = tf.gather(flat_input_tensor, flat_positions)\n",
    "    output_tensor = tf.reshape(output_tensor, [batch_size, -1, d_model])\n",
    "\n",
    "    return output_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1849af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_vocab_size, name):\n",
    "        super(MaskedLanguageModel, self).__init__()\n",
    "        self._name = name\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        self.activation1 = GELU()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.decoder = tf.keras.layers.Dense(input_vocab_size, use_bias=False)\n",
    "\n",
    "        b_init = tf.zeros_initializer()(shape=(input_vocab_size,), dtype=\"float32\")\n",
    "        self.bias = tf.Variable(initial_value=b_init, trainable=True)\n",
    "\n",
    "        self.activation2 = tf.keras.layers.Activation(activation=\"softmax\")\n",
    "\n",
    "    def call(self, x, positions):\n",
    "        # x: (batch_size, input_seq_len, d_model)\n",
    "        # positions: (batch_size, max_pred)\n",
    "\n",
    "        mask_inputs = gather_indexes(x, positions) # (batch_size, max_pred, d_model)\n",
    "\n",
    "        mask_inputs = self.dense(mask_inputs) # (batch_size, max_pred, d_model)\n",
    "        mask_inputs = self.activation1(mask_inputs)\n",
    "        mask_inputs = self.layernorm(mask_inputs)\n",
    "\n",
    "        out = self.decoder(mask_inputs) + self.bias # (batch_size, max_pred, input_vocab_size)\n",
    "\n",
    "        out = self.activation2(out)\n",
    "\n",
    "        return out # (batch_size, max_pred, input_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed44853",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86706587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, rate, name):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        self._name = name\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.dense1 = tf.keras.layers.Dense(d_model, activation='tanh')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.dense1(x)  \n",
    "        x = self.flatten(x)  \n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)  \n",
    "\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToStructModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, input_vocab_size, name):\n",
    "        super(SeqToStructModel, self).__init__()\n",
    "        self._name = name\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        self.activation1 = GELU()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.decoder = tf.keras.layers.Dense(input_vocab_size, use_bias=False)\n",
    "\n",
    "        b_init = tf.zeros_initializer()(shape=(input_vocab_size,), dtype=\"float32\")\n",
    "        self.bias = tf.Variable(initial_value=b_init, trainable=True)\n",
    "\n",
    "        self.activation2 = tf.keras.layers.Activation(activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x, positions):\n",
    "        \n",
    "\n",
    "        mask_inputs = gather_indexes(x, positions) \n",
    "\n",
    "        mask_inputs = self.dense(mask_inputs) \n",
    "        mask_inputs = self.activation1(mask_inputs)\n",
    "        mask_inputs = self.layernorm(mask_inputs)\n",
    "\n",
    "        out = self.decoder(mask_inputs) + self.bias \n",
    "\n",
    "        out = self.activation2(out)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93aba0a",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa348ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the variable learning rate formula\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b220d150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting learning rate change curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model = 10)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906c250",
   "metadata": {},
   "source": [
    "## Part3：pre-training Step 1: Train  MLM and BFP pre-training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "632d6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer=4\n",
    "input_vocab_size = 4**kmer + 2 \n",
    "maxlen = 500\n",
    "max_pred = 75\n",
    "\n",
    "num_layers=6\n",
    "d_model=16\n",
    "num_heads=2\n",
    "dff=5\n",
    "rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8efab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting pre-trained data\n",
    "mask_sequence, mask_tokens, mask_pos, labels = init_data_pre_training(kmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500889f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence = tf.keras.layers.Input(shape=(maxlen,), name=\"input_mask_sequence\")\n",
    "pos = tf.keras.layers.Input(shape=(max_pred,), dtype=tf.int32, name=\"input_mask_pos\")\n",
    "mask = create_padding_mask(sequence)\n",
    "\n",
    "# Embedding\n",
    "x = Embedding(d_model=d_model, input_vocab_size=input_vocab_size)(sequence)\n",
    "# Encoder\n",
    "bert_output = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, maxlen=maxlen, rate=rate)(x, mask)\n",
    "# MLM  TASK\n",
    "mlm_output = MaskedLanguageModel(d_model=d_model, input_vocab_size=input_vocab_size, name=\"mlm\")(bert_output, pos)\n",
    "# BFP TASK \n",
    "x = Bidirectional(GRU(16, dropout=0.2, return_sequences=True), name=\"bi-gru\")(bert_output)\n",
    "BFP_output = OutputLayer(d_model=d_model, rate=rate, name =\"rpi\")(x)\n",
    "\n",
    "# BERT-Net\n",
    "model = tf.keras.models.Model(inputs=[sequence, pos], outputs=[mlm_output, BFP_output])\n",
    "model.summary()\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-6)\n",
    "model.compile(loss={'mlm': 'sparse_categorical_crossentropy', 'rpi': 'mean_squared_error'}, loss_weights={'mlm': 1, 'rpi': 1},optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "mini_batches = 128\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'rpi_loss', mode='min', min_delta = 0, patience = 1, verbose = 1)\n",
    "history = model.fit([mask_sequence, mask_pos], [mask_tokens, labels], batch_size = mini_batches, epochs=num_epochs, callbacks = earlystop, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b774dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization saves model parameters\n",
    "checkpoint_path = \"Your own address.\"\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt_save_path = ckpt_manager.save()\n",
    "print ('Saving checkpoint at {}'.format(ckpt_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5eb7d",
   "metadata": {},
   "source": [
    "## Part4：pre-training Step 2: Train MLM, BFP and SSC pre-training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc04d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting pre-trained data\n",
    "mask_sequence, mask_tokens, mask_pos, labels = init_data_pre_training(kmer)\n",
    "struct_ped, struct_pos = init_data_pre_training_only_struct(kmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afac7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tf.keras.layers.Input(shape=(maxlen,), name=\"input_mask_sequence\")\n",
    "pos = tf.keras.layers.Input(shape=(max_pred,), dtype=tf.int32, name=\"input_mask_pos\")\n",
    "pos_struct= tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32, name=\"input_struct_pos\")\n",
    "mask = create_padding_mask(sequence)\n",
    "\n",
    "# Embedding\n",
    "x = Embedding(d_model=d_model, input_vocab_size=input_vocab_size, name='embedding')(sequence)\n",
    "# Encoder\n",
    "bert_output = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, maxlen=maxlen, rate=rate,name='encoder')(x, mask)\n",
    "# MLM  TASK\n",
    "mlm_output = MaskedLanguageModel(d_model=d_model, input_vocab_size=input_vocab_size, name=\"mlm\")(bert_output, pos)\n",
    "# BFP TASK     \n",
    "x = Bidirectional(GRU(16, dropout=0.2, return_sequences=True), name=\"bi-gru\")(bert_output)\n",
    "BFP_output = OutputLayer(d_model=d_model, rate=rate, name =\"rpi\")(x)\n",
    "#SSC TASK \n",
    "seq_to_struct_output = SeqToStructModel(d_model=d_model, input_vocab_size=input_struct_vocab_size, name=\"rna_structure\")(bert_output,pos_struct)\n",
    "\n",
    "# BERT-Net\n",
    "model = tf.keras.models.Model(inputs=[sequence, pos, pos_struct], outputs=[mlm_output,BFP_output,seq_to_struct_output])\n",
    "model.summary()\n",
    "\n",
    "learning_rate = CustomSchedule(d_model=d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-6)\n",
    "model.compile(loss={'mlm': 'sparse_categorical_crossentropy','rna_structure':'sparse_categorical_crossentropy',  'rpi': 'mean_squared_error'}, loss_weights={'mlm': 1, 'rpi': 1,'rna_structure':1 },optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad50d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-training weights\n",
    "checkpoint_path=\"Your own address.\"\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "mini_batches = 128\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor = 'rpi_loss', mode='min', min_delta = 0, patience = 1, verbose = 1)\n",
    "history = model.fit([mask_sequence, mask_pos, struct_pos], [mask_tokens, labels, struct_ped], batch_size = mini_batches, epochs=num_epochs, callbacks = earlystop, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bert_model model\n",
    "bert_model = tf.keras.models.Model(inputs=sequence, outputs=BFP_output)\n",
    "bert_model.save_weights(\"Your own address.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0986",
   "metadata": {},
   "source": [
    "## Part5：fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd7e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 4**kmer + 2 \n",
    "maxlen = 500\n",
    "\n",
    "num_layers=6\n",
    "d_model=16\n",
    "num_heads=2\n",
    "dff=5\n",
    "rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for datasetname in datasetnames:\n",
    "    \n",
    "    training_seq, training_labels, testing_seq, testing_labels = init_data_fine_tuning(datasetname=datasetname,kmer=kmer)\n",
    "    \n",
    "    sequence = tf.keras.layers.Input(shape=(maxlen,), name=\"input_mask_sequence\")\n",
    "    mask = create_padding_mask(sequence) \n",
    "    # Embedding\n",
    "    x = Embedding(d_model=d_model, input_vocab_size=input_vocab_size, name='embedding_fine')(sequence)\n",
    "    # Encoder\n",
    "    x = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, maxlen=maxlen, rate=rate,name='encoder_fine')(x, mask)\n",
    "    # BFP TASK\n",
    "    x = Bidirectional(GRU(16, dropout=0.2, return_sequences=True), name=\"bi-gru\")(x)\n",
    "    BFP_output = OutputLayer(d_model=d_model, rate=rate, name =\"rpi\")(x)\n",
    "\n",
    "    # BERT-Net\n",
    "    model = tf.keras.models.Model(inputs=sequence, outputs=BFP_output)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Initialize model parameters\n",
    "    model.load_weights(\"Your own address.\")\n",
    "    \n",
    "    # Define the model save path\n",
    "    save_directory = \"Your own address.\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    checkpoint_path = save_directory+\"/best_model.h5\"\n",
    "    \n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=8)    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor=\"val_loss\",  \n",
    "        save_best_only=True,  \n",
    "        save_weights_only=True,  \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    num_epochs = 50\n",
    "    mini_batches = 128\n",
    "    history = model.fit(\n",
    "        training_seq, \n",
    "        training_labels, \n",
    "        batch_size=mini_batches, \n",
    "        epochs=num_epochs, \n",
    "        verbose=1, \n",
    "        validation_data=(testing_seq, testing_labels), \n",
    "        callbacks=[checkpoint,early_stopping]\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb17082",
   "metadata": {},
   "source": [
    "## Part6：Evaluation of fine-tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tf.keras.layers.Input(shape=(maxlen,), name=\"input_mask_sequence\")\n",
    "mask = create_padding_mask(sequence) \n",
    "# Embedding\n",
    "x = Embedding(d_model=d_model, input_vocab_size=input_vocab_size, name='embedding_fine')(sequence)\n",
    "# Encoder\n",
    "x = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, maxlen=maxlen, rate=rate,name='encoder_fine')(x, mask)\n",
    "# BFP TASK\n",
    "x = Bidirectional(GRU(16, dropout=0.2, return_sequences=True), name=\"bi-gru\")(x)\n",
    "BFP_output = OutputLayer(d_model=d_model, rate=rate, name =\"rpi\")(x)\n",
    "\n",
    "# BERT-Net\n",
    "model = tf.keras.models.Model(inputs=sequence, outputs=BFP_output)\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasetname in datasetnames:\n",
    "    print(\"dataset:\",datasetname)\n",
    "    \n",
    "    training_seq, training_labels, testing_seq, testing_labels = init_data_fine_tuning(datasetname=datasetname,kmer=kmer)\n",
    "    \n",
    "    checkpoint_path = \"Your own address.\"    \n",
    "    model.load_weights(checkpoint_path)\n",
    "        \n",
    "    BFP_output = model.predict(x=testing_seq)\n",
    "\n",
    "    auc = roc_auc_score(testing_labels, BFP_output)\n",
    "    auc=round(auc, 3)\n",
    "    print(\"AUC =\", auc)\n",
    "    \n",
    "\n",
    "    ap = average_precision_score(testing_labels, BFP_output)\n",
    "    ap=round(ap, 3)\n",
    "    print(\"AP =\", ap)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
